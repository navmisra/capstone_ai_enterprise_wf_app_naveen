{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-candidate"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "<img src=\"./static/owl.png\" alt=\"Aavail-logo\" align=\"center\" style=\"width: 100px;\"/> **IBM AI ENTERPRISE WORKFLOW CERTIFICATION**  \n",
    "\n",
    "\n",
    "\n",
    "<hr />\n",
    "\n",
    "# Capstone Project - Part 1 \n",
    "#### Introduction and Exporatory Data Analysis (Data Investigation)\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 1.  Assimilate the business scenario and articulate testable hypothesis\n",
    "\n",
    "We will be working for a company called AAVAIL which provides a digital content streaming service similar to Netflix, Amazon Prime, and Disney+. Their value-add is a unique technology that allows them to offer local, national and international news to its subscribers in 12 languages.\n",
    "\n",
    "In a move to expand biz worldwide, AAVAIL started services in Singapore (apart from presense in other countries) with a good start for few initial months but fail to maintain the pace of revenue growth afterwards. This brings the business opportunity for data science team to analyse this business scenario, probe/investigate all kinds of sources of info and try collecting relevant data and perform series of experiments using various data-science techniques to come up with a revenue forecasting model i.e. predict or determine a reasonable value of revenue for next/future month(s) (this can be for a particular country or organisation as a whole)\n",
    "\n",
    "#### During investigative study about this scenario, we seek answers ...\n",
    "\n",
    "•\tAre there similar declines in market values elsewhere i.e. in other countries where AAVAIL operates in and see if any services quality degradation?\n",
    "\n",
    "•\tIs there any competitor jumps in, with more values to customers/users?\n",
    "\n",
    "•\tCan we use marketing to reduce the rate of churn?\n",
    "\n",
    "•\tCan we salvage the Singapore market with new products?\n",
    "\n",
    "•\tAre there factors outside of our influence that caused the situation in Singapore and is it temporary?\n",
    "\n",
    "•\tCan we identify the underlying variables in Singapore that are related to churn and can we use the knowledge to remedy the situations?\n",
    "\n",
    "    "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 2.\tState the ideal data to address the business opportunity and clarify the rationale for needing specific data.\n",
    "\n",
    "As to target to estimate revenue for Aavail in a country specific and for overall organization for next month or coming months, we may look at the past historical revenue data for each countries which can be picked from invoices generated in past for each of its customer for each streams they watched, this level of granular data can be achieved from its core billing systems.\n",
    "This data should have invoice number, date, stream, customer number and how many times any stream has been viewed and at what cost/price.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 3.\tCreate a python script to extract relevant data from multiple data sources, automating the process of data ingestion.\n",
    "\n",
    "The data collected is in the form of JSON file(s) \n",
    "To load data from these JSON files into plan flat structure i.e. CSV by running ...\n",
    "\n",
    "ingestion.py\" \"T\" \n",
    "\n",
    "later we can use same python script for ingesting data while evaluation performance drift from \"cs-production\" folder by passing CLI argument \"P\" (ingestion.py\" \"P\" )\n",
    "\n",
    "This script automatically create countries wise CSV files and one for overall.\n",
    "For this assigment purpose, we are creating only for Top 10 countries basis total revenue.\n",
    "\n",
    "Target for prediticing revenue for the next day(s), sum of 30days revenue can be treated as the target of next month.\n",
    "Hence we transform the original dataset which can be used for input dataset for modeling. This Transformed dataset is aggregated on daily basis, with unique no of invoices, unique no of streams , purchases (total count), total_views and revenue.\n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "##  4. Investigate the relationship between the relevant data, the target and the business metric."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Basic Insights of original dataset \n",
    "**********************************\n",
    "\n",
    "a) negative/missing/zero values:\n",
    "    \n",
    "    1.  Price column : only 3 negative values out of 815011 entries which can be ignored. \n",
    "        Also only 0.7% (5k) records are havinng values as Zero\n",
    "    2.  times_viewed : only 0.9% (7k) records are havinng values as Zero\n",
    "b) duplicate rows : 28k (3.5%)    \n",
    "c) data is of rage 2017-11-28 to 2019-07-31. (611 days overall, out of which 116 (19%) days of data missing).\n",
    " Daily aggregated dataset is having all dates for this range but is having 0 values beacuse of above reason (as gaps clearly shown in below fig).       \n",
    "\n",
    "<img src=\"./visualization_images/date_wise_price_views.png\"  align=\"center\" style=\"width: 700px; hieght: 700px;\"/>    \n",
    "\n",
    "\n",
    "\n",
    "d) Following is the visualization of top 10 countries by overall price(i.e revenue)    \n",
    "    \n",
    "   It is clear that AAVAIL is making revneue in UK mostly (around 92%), rest of top 9 countries having very low shares in revenue.    \n",
    "<img src=\"./visualization_images/Top10_2xaxis.png\"  align=\"center\" style=\"width: 700px; hieght: 700px;\"/>\n",
    "\n",
    "e) Target for prediticing revenue for the next day(s), sum of 30days revenue can be treated as the target of next month.\n",
    "Hence we transform the original dataset which can be used for input dataset for modeling \n",
    "\n",
    "f) Correlations plot between different variavbles -- showing mostly positive co-relation between each other, Purchase and Views are most corelated.   \n",
    "<img src=\"./visualization_images/correlations_all.png\" align=\"center\" style=\"width: 500px; hieght: 500px;\"/>\n",
    "\n",
    "\n",
    "g)   Following plot show monthly trend of country wise distribution of Revenue       \n",
    "<img src=\"./visualization_images/Revenue_Dist_CountryWise_MonthyTrend.png\"  align=\"center\" style=\"width: 700px; hieght: 700px;\"/>   \n",
    "\n",
    "h)   Following plot show week-days trend of Revenue       \n",
    "<img src=\"./visualization_images/Weekdays_revenue.png\"  align=\"center\" style=\"width: 700px; hieght: 700px;\"/>   \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 5. Articulate your findings using a deliverable with visualizations.\n",
    "\n",
    "\n",
    "** We have find following facts about the data apart from points mentioned in above section.    \n",
    "** These facts may be influencing to our Target - Revenue for a country or as a whole for Organization   \n",
    "\n",
    "\n",
    ">> Daily data is for a very limited period i.e. around 21 months - span over 2017 to 2019   \n",
    ">> Some kind of seasonality in daily revenues    \n",
    ">> 90%+ revenue is nging generated for UK, with other countries are having very negligible share.   \n",
    ">> Revenues are positively correlated with the purchases   \n",
    ">> Almost no data is availble for Saturdays throughout each country.  (86 out of 116 days missing are Saturdays)\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Capstone Project - Part 2 \n",
    "#### Model Building and Selection\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "A time series is a sequence of numerical data points in successive order. A time series tracks the movement of the chosen data points,such as a security's price, over a specified period of time with data points recorded at regular intervals.\n",
    "\n",
    "Our data, as we have seen, is a time series data where revenues (along with other paramters) arranged in regular period of days. The goal of this assigment is **time-series forecasting**, which can be thought of as the use of a model to predict future values based on previously observed values. So we will have to predict revenue which is the feature most closely related to our business opportunity.\n",
    "\n",
    "### Deliverable goals\n",
    "\n",
    "1.  State the different modeling approaches that you will compare to address the business opportunity.\n",
    "\n",
    "2.  Iterate on your suite of possible models by modifying data transformations, pipeline architectures, hyperparameters and other relevant factors.\n",
    "\n",
    "3.  Re-train your model on all of the data using the selected approach and prepare it for deployment.\n",
    "\n",
    "4.  Articulate your findings in a summary report.\n",
    "\n",
    "**HINT** : To standardize the approach across models ensure that your model predicts the next 30 days for any given point in time. For supervised learning, the total revenue over the next thirty days can be represented as a single number."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## On time-series analysis\n",
    "\n",
    "We have used TensorFlow, scikit-learn, and Spark ML as the main ways to implement models. Time-series analysis has been around a long time and there are a number of specialized packages and software to help facilitate model implementation. In the case of our business opportunity, it is required that we predict the next point or determine a reasonable value for next month's revenue. If we only had revenue, we could engineer features with revenue for the previous day, previous week, previous month and previous three months, for example. This provides features that machine learning models such as random forests or boosting could use to capture the underlying patterns or trends in the the data. You will likely spend some time optimizing this feature engineering task on a case-by-case basis.\n",
    "\n",
    "Predicting the next element in a time-series is in line with the other machine learning tasks that we have encountered in this specialization. One caveat to this approach is that sometimes we wish to project further into the future. Although, it is not a specific request of management in the case of this business opportunity, you may want to consider forecasting multiple points into the future, say three months or more. To do this, you have two main categories of methods: 'recursive forecasting' and 'ensemble forecasting'.\n",
    "\n",
    "In recursive forecasting, you will append your predictions to the feature matrix and roll forward until you get to the desired number of forecasts in the future. In the ensemble approach, you will use separate models for each point. It is possible to use a hybridization of these two ideas as well. If you wish to take your forecasting model to the next level, try to project several months into the future with one or both of these ideas.\n",
    "\n",
    "Also, be aware that the assumptions of line regression are generally invalidated when using time-series data because of auto-correlation. The engineered features are derived mostly from revenue which often means that there is a high degree of correlation. You will get further with more sophisticated models to in combination with smartly engineered features.\n",
    "Deliverables\n",
    "Data Engineering\n",
    "\n",
    "    Engineer Features with Rolling Window: When forecasting the revenues only the realized (historical) values are known. The unrealized values are not given. Therefore for each instance in the dataset I have to come up with additional features that refer to historical values.\n",
    "\n",
    "    Target Value Rolling Window: To standardize the approach across models ensure the target is the sum of revenues over the next 30 days for any given point in time. For supervised learning, the total revenue over the next thirty days can be represented as a single number\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Features Engineering\n",
    "\n",
    "* **Engineer Features with Rolling Window**: When forecasting the revenues only the realized (historical) values are known. The unrealized values are not given. Therefore for each instance in the dataset I have to come up with additional features that refer to historical values.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "loading.......\n",
      ".....fetching data from  data\\cs-train\n",
      "... loading ts data from files\n",
      "cleaning and peforming basic dataQality checks.......\n",
      "Generating Input features for dataset:  all\n",
      "Input Variables shape:  (546, 7) \n",
      "Target variables shape:  (546,) \n",
      "Total days of data: 546\n",
      "Generating Input features for dataset:  eire\n",
      "Input Variables shape:  (549, 7) \n",
      "Target variables shape:  (549,) \n",
      "Total days of data: 549\n",
      "Generating Input features for dataset:  france\n",
      "Input Variables shape:  (549, 7) \n",
      "Target variables shape:  (549,) \n",
      "Total days of data: 549\n",
      "Generating Input features for dataset:  germany\n",
      "Input Variables shape:  (549, 7) \n",
      "Target variables shape:  (549,) \n",
      "Total days of data: 549\n",
      "Generating Input features for dataset:  hong_kong\n",
      "Input Variables shape:  (282, 7) \n",
      "Target variables shape:  (282,) \n",
      "Total days of data: 282\n",
      "Generating Input features for dataset:  netherlands\n",
      "Input Variables shape:  (548, 7) \n",
      "Target variables shape:  (548,) \n",
      "Total days of data: 548\n",
      "Generating Input features for dataset:  norway\n",
      "Input Variables shape:  (407, 7) \n",
      "Target variables shape:  (407,) \n",
      "Total days of data: 407\n",
      "Generating Input features for dataset:  portugal\n",
      "Input Variables shape:  (549, 7) \n",
      "Target variables shape:  (549,) \n",
      "Total days of data: 549\n",
      "Generating Input features for dataset:  singapore\n",
      "Input Variables shape:  (250, 7) \n",
      "Target variables shape:  (250,) \n",
      "Total days of data: 250\n",
      "Generating Input features for dataset:  spain\n",
      "Input Variables shape:  (547, 7) \n",
      "Target variables shape:  (547,) \n",
      "Total days of data: 547\n",
      "Generating Input features for dataset:  united_kingdom\n",
      "Input Variables shape:  (546, 7) \n",
      "Target variables shape:  (546,) \n",
      "Total days of data: 546\n",
      "load time: 0:01:02\n"
     ]
    }
   ],
   "source": [
    "%run DataEngineering.py T True"
   ]
  },
  {
   "source": [
    "### Models\n",
    "\n",
    "For each country a model is built and trained. \n",
    "\n",
    "1 Random Forest Regressor      \n",
    "2 Gradient Bossting Regressor\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "False\n",
      "LOADING MODELS\n",
      ".....fetching data from  data\\cs-train\n",
      "... loading ts data from files\n",
      "... models loaded:  all,eire,france,germany,hong_kong,netherlands,norway,portugal,singapore,spain,united_kingdom\n",
      "\n",
      "Test Prediction for inputs: \n",
      "country :  all \n",
      "year:  2018 \n",
      "month:  01 \n",
      "day:  05\n",
      "y_pred:  [177416.4782]\n",
      "\n",
      "METADATA\n",
      "...label:all, algorithm:RandomForestRegressor\n",
      "...label:eire, algorithm:RandomForestRegressor\n",
      "...label:france, algorithm:RandomForestRegressor\n",
      "...label:germany, algorithm:RandomForestRegressor\n",
      "...label:hong_kong, algorithm:RandomForestRegressor\n",
      "...label:netherlands, algorithm:RandomForestRegressor\n",
      "...label:norway, algorithm:RandomForestRegressor\n",
      "...label:portugal, algorithm:RandomForestRegressor\n",
      "...label:singapore, algorithm:AdaBoostRegressor\n",
      "...label:spain, algorithm:RandomForestRegressor\n",
      "...label:united_kingdom, algorithm:RandomForestRegressor\n",
      "\n",
      "Prediction result:  {'y_pred': array([177416.4782])}\n",
      "\n",
      "Overall execution time:  0:01:06\n"
     ]
    }
   ],
   "source": [
    "%run model_capstone.py False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ".....fetching data from  data\\cs-train\n... loading ts data from files\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                model\n",
       "ALL             RandomForestRegressor\n",
       "EIRE            RandomForestRegressor\n",
       "FRANCE          RandomForestRegressor\n",
       "GERMANY         RandomForestRegressor\n",
       "HONG_KONG       RandomForestRegressor\n",
       "NETHERLANDS     RandomForestRegressor\n",
       "NORWAY          RandomForestRegressor\n",
       "PORTUGAL        RandomForestRegressor\n",
       "SINGAPORE           AdaBoostRegressor\n",
       "SPAIN           RandomForestRegressor\n",
       "UNITED_KINGDOM  RandomForestRegressor"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>model</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>ALL</th>\n      <td>RandomForestRegressor</td>\n    </tr>\n    <tr>\n      <th>EIRE</th>\n      <td>RandomForestRegressor</td>\n    </tr>\n    <tr>\n      <th>FRANCE</th>\n      <td>RandomForestRegressor</td>\n    </tr>\n    <tr>\n      <th>GERMANY</th>\n      <td>RandomForestRegressor</td>\n    </tr>\n    <tr>\n      <th>HONG_KONG</th>\n      <td>RandomForestRegressor</td>\n    </tr>\n    <tr>\n      <th>NETHERLANDS</th>\n      <td>RandomForestRegressor</td>\n    </tr>\n    <tr>\n      <th>NORWAY</th>\n      <td>RandomForestRegressor</td>\n    </tr>\n    <tr>\n      <th>PORTUGAL</th>\n      <td>RandomForestRegressor</td>\n    </tr>\n    <tr>\n      <th>SINGAPORE</th>\n      <td>AdaBoostRegressor</td>\n    </tr>\n    <tr>\n      <th>SPAIN</th>\n      <td>RandomForestRegressor</td>\n    </tr>\n    <tr>\n      <th>UNITED_KINGDOM</th>\n      <td>RandomForestRegressor</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "from model_capstone import model_load\n",
    "import pandas as pd\n",
    "\n",
    "all_data, all_models = model_load(prefix='test')\n",
    "\n",
    "labels = {}\n",
    "for key, item in all_models.items():\n",
    "    labels[key.upper()] = type(item.best_estimator_[\"reg\"]).__name__\n",
    "    \n",
    "bm = pd.DataFrame.from_dict(labels, orient=\"index\", columns=[\"model\"])\n",
    "bm"
   ]
  },
  {
   "source": [
    "# Capstone Project - Part 3 \n",
    "#### Model Deployment\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "1.  Build a draft version of an API with train, predict, and logfile endpoints.\n",
    "2.  Using Docker, bundle your API, model, and unit tests.\n",
    "3.  Using test-driven development iterate on your API in a way that anticipates scale, load, and drift.\n",
    "4.  Create a post-production analysis script that investigates the relationship between model performance and the business metric.\n",
    "5.  Articulate your summarized findings in a final report."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 1.   flask API to perform Model training, prediction and logfile views"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### Run followig command from command prompt from this project folder directory\n",
    "#### python app.py\n",
    "#### Then check below url is working \n",
    "#### http://localhost:8080/"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from ast import literal_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## checking API predict endpoint\n",
    "query = {\"year\":\"2018\",\"month\":\"1\",\"day\":\"5\",\"country\":\"all\"} #,\"dev\":\"True\",\"verbose\":\"True\"\n",
    "port = 8080\n",
    "r = requests.post('http://localhost:{}/predict'.format(port),json=query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<Response [200]> {\"y_pred\":[177416.4782]}\n\n{'y_pred': [177416.4782]}\n"
     ]
    }
   ],
   "source": [
    "print(r,r.text)\n",
    "response = literal_eval(r.text)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<Response [200]> true\n\n"
     ]
    }
   ],
   "source": [
    "## checking API train endpoint\n",
    "query = {\"dev\":\"True\",\"verbose\":\"True\"}\n",
    "port = 8080\n",
    "r = requests.post('http://localhost:{}/train'.format(port),json=query)\n",
    "print(r,r.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<Response [200]>\n"
     ]
    }
   ],
   "source": [
    "## checking API logging endpoint\n",
    "port=8080\n",
    "r = requests.get('http://localhost:{}/logs/train-2020-12.log'.format(port))\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking log files by \n",
    "# !dir .\\logs\\*.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## hit this API logging endpoint in browser-- to get the logfile downloaded... \n",
    "http://localhost:8080/logs/train-2020-12.log\n",
    "# Note : Replace FileName as descried"
   ]
  },
  {
   "source": [
    "### 2.   Docker API, Unittests"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "test the train functionality\n",
      "... test flag on\n",
      "...... subsetting data\n",
      "...... subsetting countries\n",
      ".....fetching data from  data\\cs-train\n",
      "... loading ts data from files\n",
      "Model Training for :  all\n",
      "Perform a train-test split\n",
      "...training model: 2/2\n",
      "...best model:Random Forest\n",
      "Execution time:  000:00:19\n",
      "Model Training for :  united_kingdom\n",
      "Perform a train-test split\n",
      "...training model: 2/2\n",
      "...best model:Random Forest\n",
      ".Execution time:  000:00:10\n",
      "model_train test - training completed\n",
      "saved_model:  models\\test-netherlands-model-0_1.joblib\n",
      "\n",
      "test the load functionality\n",
      ".....fetching data from  data\\cs-train\n",
      "... loading ts data from files\n",
      ".\n",
      "test the predict function input\n",
      ".....fetching data from  data\\cs-train\n",
      "... loading ts data from files\n",
      ".y_pred:  [968.0145]\n",
      "\n",
      "METADATA\n",
      "...label:all, algorithm:RandomForestRegressor\n",
      "...label:eire, algorithm:RandomForestRegressor\n",
      "...label:france, algorithm:RandomForestRegressor\n",
      "...label:germany, algorithm:RandomForestRegressor\n",
      "...label:hong_kong, algorithm:RandomForestRegressor\n",
      "...label:netherlands, algorithm:RandomForestRegressor\n",
      "...label:norway, algorithm:RandomForestRegressor\n",
      "...label:portugal, algorithm:RandomForestRegressor\n",
      "...label:singapore, algorithm:RandomForestRegressor\n",
      "...label:spain, algorithm:RandomForestRegressor\n",
      "...label:united_kingdom, algorithm:RandomForestRegressor\n",
      "unit test pridict result:  [968.0145]\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 3 tests in 105.392s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "%run ./unittests/ModelTests.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.071s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "%run ./unittests/LoggerTests.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      ".test the train functionality of API, train_complete:  true\n",
      "...\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 187.141s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "%run ./unittests/ApiTests.py"
   ]
  },
  {
   "source": [
    "#### Run followig command from command prompt from this project folder directory to run ALL UNIT TEST CASES\n",
    "#### python run-tests.py"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Creating Docker Container"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### Using \"requirements.txt\" and \"Dockerfile\" ............\n",
    "\n",
    "**Build the Docker image and run it**\n",
    "\n",
    "Step one: build the image from the project directory\n",
    " \n",
    "```bash\n",
    "    ~$ docker build -t capstone-ai-wf-flask-app .\n",
    "```\n",
    "\n",
    "Check that the image is there.\n",
    "\n",
    "```bash\n",
    "    ~$ docker image ls\n",
    "```\n",
    "\n",
    "You may notice images that you no longer use.  You may delete them with\n",
    "\n",
    "```bash\n",
    "    ~$ docker image rm IMAGE_ID_OR_NAME\n",
    "```\n",
    "\n",
    "Run the container\n",
    "\n",
    "```bash\n",
    "docker run -p 9090:8080 capstone-ai-wf-flask-app\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## checking API predict endpoint\n",
    "import requests\n",
    "from ast import literal_eval\n",
    "\n",
    "query = {\"year\":\"2018\",\"month\":\"1\",\"day\":\"5\",\"country\":\"all\"} #,\"dev\":\"True\",\"verbose\":\"True\"\n",
    "port = 9090\n",
    "r = requests.post('http://localhost:{}/predict'.format(port),json=query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<Response [200]> {\"y_pred\":[177416.4782]}\n\n{'y_pred': [177416.4782]}\n"
     ]
    }
   ],
   "source": [
    "print(r,r.text)\n",
    "response = literal_eval(r.text)\n",
    "print(response)"
   ]
  },
  {
   "source": [
    "### 4.   Post Production "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ".....fetching data from  data\\cs-train\n",
      "... loading ts data from files\n",
      "outlier_X 2.2\n",
      "wasserstein_X 0.08\n",
      "wasserstein_y 3696.96\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "%run monitoring.py"
   ]
  },
  {
   "source": [
    "### 5.   Summary"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Deployment - Generaly data in real world not having the true labels so we would not be able to calculate the wasserstein distance between known and predicted targets (shown in the 3rd column). We would however be able to check for outliers on the targets. We see that the distance metrics do a very good job detecting the drift in target distribution, the test that is the most important is of course most important at the level of X, because these data will be available."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}